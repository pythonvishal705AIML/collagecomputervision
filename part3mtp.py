# -*- coding: utf-8 -*-
"""vishalmtp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f6tjErA1yfDS3fbdq-Rh-uc5VrEkrCzw

After 1) colletion of data.

2)doing preprocessing (remove background)

3) data augumentation process incrises data(for beter analysis) for this using method
"""

folder_path = '/content/wawelet decom'
folder_path

import zipfile

# Path to the zip file
zip_file_path = '/content/drive/MyDrive/wawelet decom.zip'

# Directory to extract to
extract_dir = '/content/drive/MyDrive/wawelet decom'

if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

extracted_files = os.listdir(extract_dir)
print("Files extracted successfully:", extracted_files)

import os
import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft2, fftshift
from PIL import Image
from mpl_toolkits.mplot3d import Axes3D

"""here i first try fouries transfermation methed on data(because this method not used before Aquaculture domain)"""

base_path = "/content/drive/MyDrive/wawelet decom/"
folders = ["day1", "day2", "day3", "day4"]

def load_images_from_folder(folder):
    images = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        try:
            img = Image.open(img_path)
            if img is not None:
                img = img.convert('L')                          # Convert image to grayscale
                img = np.array(img.resize((256, 256)))          # Resize image to consistent dimensions
                images.append(img)
        except Exception as e:
            print(f"Error loading image {img_path}: {e}")
    return images

images = [load_images_from_folder(os.path.join(base_path, folder)) for folder in folders]


fft_images = []                                                   ##  FAST Fourier Transform on the images for each day
for day_images in images:
    day_fft_images = [fft2(image) for image in day_images]
    fft_images.append(day_fft_images)

                                                                                          # Shift zero frequency component to center for each day
fft_shift_images = [[fftshift(fft_image) for fft_image in day_fft_images] for day_fft_images in fft_images]

plt.figure(figsize=(12, 8))

for i in range(len(folders)):
    plt.subplot(2, 2, i+1)
    if len(fft_shift_images[i]) > 0:
        plt.imshow(np.log(1 + np.abs(fft_shift_images[i][0])), cmap='gray')
        plt.title(folders[i])
    else:
        print(f"No images found for {folders[i]}")

plt.tight_layout()
plt.show()

"""for better analysis plot 3D PLOT"""

fig = plt.figure(figsize=(12, 8))

for i in range(len(folders)):
    ax = fig.add_subplot(2, 2, i+1, projection='3d')
    if len(fft_shift_images[i]) > 0:
        magnitude_spectrum = np.abs(fft_shift_images[i][0])                                  # Magnitude spectrum of the first image from each day
        x, y = np.meshgrid(range(magnitude_spectrum.shape[1]), range(magnitude_spectrum.shape[0]))
        ax.plot_surface(x, y, np.log(1 + magnitude_spectrum), cmap='viridis')
        ax.set_title(folders[i])
        ax.set_xlabel('Frequency (X)')
        ax.set_ylabel('Frequency (Y)')
        ax.set_zlabel('Log Magnitude')                                                    #INTENSITY OF FREQUANCY
    else:
        print(f"No images found for {folders[i]}")

plt.tight_layout()
plt.show()

"""CORRELATION MATRIX OF FFT"""

# Calculate cross-correlation coefficient between Fourier spectra of images from different days
cross_correlation_matrix = np.zeros((len(folders), len(folders)))
for i in range(len(folders)):
    for j in range(len(folders)):
        cross_correlation_matrix[i, j] = calculate_cross_correlation(fft_shift_images[i][0], fft_shift_images[j][0])

# Plot cross-correlation matrix
plt.figure(figsize=(8, 6))
plt.imshow(cross_correlation_matrix, cmap='viridis', interpolation='nearest')
plt.colorbar()
plt.title('Cross-Correlation Matrix of Fourier Spectra')
plt.xlabel('Day')
plt.ylabel('Day')
plt.xticks(range(len(folders)), folders)
plt.yticks(range(len(folders)), folders)
plt.tight_layout()
plt.show()





"""CALCULATE FRESHNES SCORE OF FILLET"""

import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.applications import MobileNetV2
from urllib.request import urlretrieve


weights_url = "https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5"
weights_path = "mobilenet_v2_weights.h5"

if not os.path.exists(weights_path):
    print("Downloading MobileNetV2 weights...")
    urlretrieve(weights_url, weights_path)
    print("Download completed.")


new_model = Sequential()
new_model.add(MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=weights_path))
new_model.add(Flatten())
new_model.add(Dense(units=64, activation='relu'))
new_model.add(Dense(units=1, activation='sigmoid'))


def preprocess_image(image_path):                 #preprocing
    img = cv2.imread(image_path)
    img = cv2.resize(img, (224, 224))           # Resize to match model input shape
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype('float32') / 255.0          # Normalize pixel values
    return img

day1_folder = '/content/drive/MyDrive/wawelet decom/day1'
day2_folder = '/content/drive/MyDrive/wawelet decom/day2'
day3_folder = '/content/drive/MyDrive/wawelet decom/day3'
day4_folder = '/content/drive/MyDrive/wawelet decom/day4'


def predict_freshness(day_folder):                  #prediction
    freshness_scores = []
    for filename in os.listdir(day_folder):
        if filename.endswith('.jpg') or filename.endswith('.png'):
            image_path = os.path.join(day_folder, filename)
            preprocessed_img = preprocess_image(image_path)
            prediction = new_model.predict(np.expand_dims(preprocessed_img, axis=0))
            freshness_scores.append(prediction[0][0])
    return freshness_scores

day1_scores = predict_freshness(day1_folder)       #calculating
day2_scores = predict_freshness(day2_folder)
day3_scores = predict_freshness(day3_folder)
day4_scores = predict_freshness(day4_folder)

# Calculate average freshness change from day 1 to day 4
average_change = np.mean(day4_scores) - np.mean(day1_scores)

print(f"Average freshness change from day 1 to day 4: {average_change:.2f}")

# Assuming you have scores and days variables containing freshness scores and corresponding day labels
scores = [day1_scores, day2_scores, day3_scores, day4_scores]
days = ['Day 1', 'Day 2', 'Day 3', 'Day 4']

plt.figure(figsize=(10, 6))
plt.boxplot(scores, labels=days)
plt.title('Freshness Scores Distribution')
plt.xlabel('Day')
plt.ylabel('Freshness Score')
plt.show()



plt.figure(figsize=(10, 6))
plt.violinplot(scores, showmedians=True, showextrema=True)
plt.xticks([1, 2, 3, 4], days)
plt.xlabel('Day')
plt.ylabel('Moisture Contents')
plt.title('Distribution of Freshness Scores for Each Day')
plt.grid(True)
plt.show()



import seaborn as sns

plt.figure(figsize=(10, 6))
for i, day_scores in enumerate(scores):
    sns.kdeplot(day_scores, label=days[i])
plt.xlabel('Freshness Score')
plt.ylabel('Density')
plt.title('Kernel Density Estimate (KDE) of Freshness Scores')
plt.legend()
plt.grid(True)
plt.show()



new_model = Sequential()
new_model.add(MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights=weights_path))
new_model.add(Flatten())
# new_model.add(Dense(units=64, activation='relu'))                                               change in structure
new_model.add(Dense(units=1, activation='sigmoid'))

# Define a function to preprocess an image
def preprocess_image(image_path):
    img = cv2.imread(image_path)
    img = cv2.resize(img, (224, 224))                                                   # Resize to match model input shape
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype('float32') / 255.0                                                  # Normalize pixel values
    return img

day1_folder = '/content/drive/MyDrive/wawelet decom/day1'
day2_folder = '/content/drive/MyDrive/wawelet decom/day2'
day3_folder = '/content/drive/MyDrive/wawelet decom/day3'
day4_folder = '/content/drive/MyDrive/wawelet decom/day4'

                                                                                      # Process images and make predictions
def predict_freshness(day_folder):
    freshness_scores = []
    for filename in os.listdir(day_folder):
        if filename.endswith('.jpg') or filename.endswith('.png'):
            image_path = os.path.join(day_folder, filename)
            preprocessed_img = preprocess_image(image_path)
            prediction = new_model.predict(np.expand_dims(preprocessed_img, axis=0))
            freshness_scores.append(prediction[0][0])
    return freshness_scores

#freshness scores
day1_scores = predict_freshness(day1_folder)
day2_scores = predict_freshness(day2_folder)
day3_scores = predict_freshness(day3_folder)
day4_scores = predict_freshness(day4_folder)

average_change = np.mean(day4_scores) - np.mean(day1_scores)

print(f"Average freshness change from day 1 to day 4: {average_change:.2f}")



import matplotlib.pyplot as plt
scores = [day1_scores, day2_scores, day3_scores, day4_scores]
plt.figure(figsize=(10, 6))
plt.violinplot(scores, showmedians=True, showextrema=True)
plt.xticks([1, 2, 3, 4], days)
plt.xlabel('Day')
plt.ylabel('Freshness Score')
plt.title('Distribution of Freshness Scores for Each Day')
plt.grid(True)
plt.show()



"""use PCA on data for analyse"""

from skimage import io
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from skimage.transform import resize



def load_spectra(folder_path, target_shape=(100, 100)):
    spectra = []
    for file_name in os.listdir(folder_path):
        if file_name.endswith('.jpg') or file_name.endswith('.png'):
            img_path = os.path.join(folder_path, file_name)
            img = io.imread(img_path)
            img_resized = resize(img, target_shape)                        # Resize the image to a common shape
            spectra.append(img_resized)
    return np.array(spectra)


def preprocess_spectra(spectra):
    average_spectrum = np.mean(spectra, axis=0)                           #calculated average spectrum,1D array
                                                                          #is the average of the corresponding column in the input spectra.
    return average_spectrum

all_spectra = []
for folder_path in folder_paths:
    spectra = load_spectra(folder_path)
    average_spectrum = preprocess_spectra(spectra)
    all_spectra.append(average_spectrum)


X = np.concatenate(all_spectra)
labels = np.repeat(np.arange(len(folder_paths)), [s.shape[0] for s in all_spectra])

# Perform PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)


plt.figure(figsize=(10, 6))
for label in np.unique(labels):
    plt.scatter(X_pca[labels == label, 0], X_pca[labels == label, 1], label=f"Day {label+1}")
plt.title('PCA Scatter Plot')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.grid(True)
plt.show()

loadings = pca.components_[0]
plt.figure(figsize=(8, 6))
plt.plot(loadings)
plt.title('Loadings Plot on PC1')
plt.xlabel('Wavelength')
plt.ylabel('Loadings')
plt.grid(True)
plt.show()



"""try to clasify day wise data using vgg16 model for feature extraction"""

import torch
import torchvision.models as models
from torchvision import transforms
from PIL import Image
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vgg16 = models.vgg16(pretrained=True).to(device)                                # Move model to device
vgg16.eval()


def preprocess_image(image_path):
    image = Image.open(image_path).convert('RGB')                           # Convert to RGB
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    return preprocess(image).unsqueeze(0).to(device)


def extract_vgg_features(image_path):                                  # Function to extract features from an image using VGG
    image = preprocess_image(image_path)
    with torch.no_grad():
        features = vgg16.features(image)
    return features.cpu().numpy().flatten()                           # Remove gradient, move to CPU, and flatten


day_folders = [
    '/content/drive/MyDrive/wawelet decom/day1',
    '/content/drive/MyDrive/wawelet decom/day2',
    '/content/drive/MyDrive/wawelet decom/day3',

]

X = []
y = []
for day_folder in day_folders:
    for image_file in os.listdir(day_folder):
        image_path = os.path.join(day_folder, image_file)
        try:
            features = extract_vgg_features(image_path)
            X.append(features)
            y.append(int(day_folder.split('/')[-1].lstrip('day')))                # Extract numeric part

        except (IOError, OSError) as e:
            print(f"Error processing image {image_path}: {e}")


X = np.array(X)
y = np.array(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

cm = confusion_matrix(y_test, y_pred)
cm